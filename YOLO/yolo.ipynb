{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import tvm\n",
    "from tvm import relay, relax\n",
    "import numpy as np\n",
    "from tvm.script import tir as T\n",
    "from tvm.script import relax as R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"image\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"None\"\n",
      "        denotation: \"DATA_BATCH\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 3\n",
      "        denotation: \"DATA_CHANNEL\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 416\n",
      "        denotation: \"DATA_FEATURE\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 416\n",
      "        denotation: \"DATA_FEATURE\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  denotation: \"IMAGE\"\n",
      "}\n",
      "doc_string: \"Input image. Image(s) in RGB format. It is a [N, C, H, W]-tensor. The 1st/2nd/3rd slices along the C-axis are red, green, and blue channels, respectively.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"tinyyolov2-8.onnx\", \"rb\") as f:\n",
    "    onnx_model = onnx.load(f)\n",
    "\n",
    "print(onnx_model.graph.input[0])\n",
    "\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, {\"image\": (1,3,416,416)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.relax.testing.relay_translator import from_relay\n",
    "\n",
    "relax_mod = from_relay(mod[\"main\"], target=\"llvm\")\n",
    "#relax_mod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name                          Duration (us)  Percent  Device  Count                                                                          Argument Shapes  \n",
       "contrib_conv2d_NCHWc7          3 038 160,84    46.56    cpu0      1    float32[1, 256, 15, 15, 4], float32[256, 256, 3, 3, 4, 4], float32[1, 256, 13, 13, 4]  \n",
       "contrib_conv2d_NCHWc6          1 533 685,92    23.50    cpu0      1    float32[1, 128, 15, 15, 4], float32[256, 128, 3, 3, 4, 4], float32[1, 256, 13, 13, 4]  \n",
       "contrib_conv2d_NCHWc4            404 158,68     6.19    cpu0      1        float32[1, 32, 28, 28, 4], float32[64, 32, 3, 3, 4, 4], float32[1, 64, 26, 26, 4]  \n",
       "contrib_conv2d_NCHWc5            390 758,51     5.99    cpu0      1      float32[1, 64, 15, 15, 4], float32[128, 64, 3, 3, 4, 4], float32[1, 128, 13, 13, 4]  \n",
       "contrib_conv2d_NCHWc3            369 654,51     5.66    cpu0      1        float32[1, 16, 54, 54, 4], float32[32, 16, 3, 3, 4, 4], float32[1, 32, 52, 52, 4]  \n",
       "contrib_conv2d_NCHWc2            350 578,98     5.37    cpu0      1      float32[1, 8, 106, 106, 4], float32[16, 8, 3, 3, 4, 4], float32[1, 16, 104, 104, 4]  \n",
       "contrib_conv2d_NCHWc1            335 163,58     5.14    cpu0      1        float32[1, 4, 210, 210, 4], float32[8, 4, 3, 3, 4, 4], float32[1, 8, 208, 208, 4]  \n",
       "contrib_conv2d_NCHWc8             40 037,44     0.61    cpu0      1  float32[1, 1024, 13, 13, 1], float32[125, 1024, 1, 1, 1, 1], float32[1, 125, 13, 13, 1]  \n",
       "contrib_conv2d_NCHWc              32 410,70     0.50    cpu0      1        float32[1, 1, 418, 418, 3], float32[4, 1, 3, 3, 3, 4], float32[1, 4, 416, 416, 4]  \n",
       "leaky_relu                         5 772,97     0.09    cpu0      1                                   float32[1, 4, 416, 416, 4], float32[1, 4, 416, 416, 4]  \n",
       "add1                               4 523,16     0.07    cpu0      1           float32[1, 4, 416, 416, 4], float32[1, 4, 1, 1, 4], float32[1, 4, 416, 416, 4]  \n",
       "max_pool2d                         2 115,32     0.03    cpu0      1                                   float32[1, 4, 416, 416, 4], float32[1, 4, 208, 208, 4]  \n",
       "leaky_relu2                        1 516,04     0.02    cpu0      1                                 float32[1, 16, 104, 104, 4], float32[1, 16, 104, 104, 4]  \n",
       "add2                               1 418,06     0.02    cpu0      1           float32[1, 8, 208, 208, 4], float32[1, 8, 1, 1, 4], float32[1, 8, 208, 208, 4]  \n",
       "leaky_relu1                        1 251,38     0.02    cpu0      1                                   float32[1, 8, 208, 208, 4], float32[1, 8, 208, 208, 4]  \n",
       "max_pool2d2                        1 202,08     0.02    cpu0      1                                   float32[1, 16, 104, 104, 4], float32[1, 16, 52, 52, 4]  \n",
       "multiply                           1 192,59     0.02    cpu0      1                             float32[1, 3, 416, 416], float32[1], float32[1, 3, 416, 416]  \n",
       "pad                                1 175,21     0.02    cpu0      1                              float32[1, 3, 416, 416], float32[], float32[1, 3, 418, 418]  \n",
       "add3                                 856,77     0.01    cpu0      1        float32[1, 16, 104, 104, 4], float32[1, 16, 1, 1, 4], float32[1, 16, 104, 104, 4]  \n",
       "pad2                                 851,65     0.01    cpu0      1                        float32[1, 4, 208, 208, 4], float32[], float32[1, 4, 210, 210, 4]  \n",
       "layout_transform                     767,22     0.01    cpu0      1                                      float32[1, 3, 418, 418], float32[1, 1, 418, 418, 3]  \n",
       "add                                  757,80     0.01    cpu0      1                       float32[3, 1, 1], float32[1, 3, 416, 416], float32[1, 3, 416, 416]  \n",
       "max_pool2d1                          728,17     0.01    cpu0      1                                   float32[1, 8, 208, 208, 4], float32[1, 8, 104, 104, 4]  \n",
       "leaky_relu3                          620,19     0.01    cpu0      1                                     float32[1, 32, 52, 52, 4], float32[1, 32, 52, 52, 4]  \n",
       "add4                                 590,50     0.01    cpu0      1            float32[1, 32, 52, 52, 4], float32[1, 32, 1, 1, 4], float32[1, 32, 52, 52, 4]  \n",
       "max_pool2d3                          480,66     0.01    cpu0      1                                     float32[1, 32, 52, 52, 4], float32[1, 32, 26, 26, 4]  \n",
       "pad6                                 466,15     0.01    cpu0      1                          float32[1, 16, 52, 52, 4], float32[], float32[1, 16, 54, 54, 4]  \n",
       "add7                                 413,14     0.01    cpu0      2         float32[1, 256, 13, 13, 4], float32[1, 256, 1, 1, 4], float32[1, 256, 13, 13, 4]  \n",
       "leaky_relu6                          304,81     0.00    cpu0      2                                   float32[1, 256, 13, 13, 4], float32[1, 256, 13, 13, 4]  \n",
       "pad4                                 252,06     0.00    cpu0      1                        float32[1, 8, 104, 104, 4], float32[], float32[1, 8, 106, 106, 4]  \n",
       "add5                                 144,41     0.00    cpu0      1            float32[1, 64, 26, 26, 4], float32[1, 64, 1, 1, 4], float32[1, 64, 26, 26, 4]  \n",
       "pad8                                 125,67     0.00    cpu0      1                          float32[1, 32, 26, 26, 4], float32[], float32[1, 32, 28, 28, 4]  \n",
       "layout_transform1                    112,91     0.00    cpu0      1                                  float32[1, 256, 13, 13, 4], float32[1, 1024, 13, 13, 1]  \n",
       "leaky_relu4                          112,60     0.00    cpu0      1                                     float32[1, 64, 26, 26, 4], float32[1, 64, 26, 26, 4]  \n",
       "pad13                                 67,54     0.00    cpu0      1                        float32[1, 256, 13, 13, 4], float32[], float32[1, 256, 15, 15, 4]  \n",
       "add8                                  49,18     0.00    cpu0      1         float32[1, 125, 13, 13, 1], float32[1, 125, 1, 1, 1], float32[1, 125, 13, 13, 1]  \n",
       "add6                                  48,07     0.00    cpu0      1         float32[1, 128, 13, 13, 4], float32[1, 128, 1, 1, 4], float32[1, 128, 13, 13, 4]  \n",
       "max_pool2d4                           40,07     0.00    cpu0      1                                     float32[1, 64, 26, 26, 4], float32[1, 64, 13, 13, 4]  \n",
       "leaky_relu5                           33,68     0.00    cpu0      1                                   float32[1, 128, 13, 13, 4], float32[1, 128, 13, 13, 4]  \n",
       "max_pool2d5                           32,59     0.00    cpu0      1                                   float32[1, 128, 14, 14, 4], float32[1, 128, 13, 13, 4]  \n",
       "pad12                                 30,91     0.00    cpu0      1                        float32[1, 128, 13, 13, 4], float32[], float32[1, 128, 15, 15, 4]  \n",
       "pad11                                 27,81     0.00    cpu0      1                        float32[1, 128, 13, 13, 4], float32[], float32[1, 128, 14, 14, 4]  \n",
       "pad10                                 26,69     0.00    cpu0      1                          float32[1, 64, 13, 13, 4], float32[], float32[1, 64, 15, 15, 4]  \n",
       "layout_transform2                     22,99     0.00    cpu0      1                                      float32[1, 125, 13, 13, 1], float32[1, 125, 13, 13]  \n",
       "vm.builtin.reshape                    11,05     0.00    cpu0      1                                                              float32[1, 16, 104, 104, 4]  \n",
       "vm.builtin.reshape                    10,75     0.00    cpu0      1                                                                float32[1, 64, 26, 26, 4]  \n",
       "vm.builtin.reshape                     8,36     0.00    cpu0      1                                                                float32[1, 32, 52, 52, 4]  \n",
       "vm.builtin.reshape                     7,03     0.00    cpu0      1                                                               float32[1, 4, 416, 416, 4]  \n",
       "vm.builtin.check_tensor_info           6,29     0.00    cpu0      1                                                                  float32[1, 3, 416, 416]  \n",
       "vm.builtin.match_shape                 6,01     0.00    cpu0      1                                                                  float32[1, 3, 416, 416]  \n",
       "vm.builtin.reshape                     4,16     0.00    cpu0      1                                                               float32[1, 8, 208, 208, 4]  \n",
       "vm.builtin.reshape                     3,73     0.00    cpu0      1                                                               float32[1, 256, 13, 13, 4]  \n",
       "----------                                                                                                                                                    \n",
       "Sum                            6 522 797,63    99.95             54                                                                                           \n",
       "Total                          6 525 866,82             cpu0      1                                                                                           \n",
       "\n",
       "Configuration\n",
       "-------------\n",
       "Number of threads: 2\n",
       "Executor: VM"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# En benchmarkant le modèle on remarque que quelques convolutions monopolisent la puissance de calcul\n",
    "\n",
    "\n",
    "x = np.random.rand(1, 3, 416, 416).astype(\"float32\")\n",
    "tvm_x = tvm.nd.array(x)\n",
    "\n",
    "ex = relax.build(relax_mod, target=\"llvm\")\n",
    "vm = relax.VirtualMachine(ex, tvm.cpu(), profile=True)\n",
    "evaluator = vm.profile(\"main\",\n",
    "    tvm_x, *params\n",
    ")\n",
    "evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implémentation relax de la conv7\n",
    "\n",
    "class RelaxConv(relax.frontend.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RelaxConv, self).__init__()\n",
    "        self.conv = relax.frontend.nn.Conv2D(1024, 1024, kernel_size=3, stride=1, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "input_shape = (1, 1024, 15, 15)\n",
    "relax_mod, _ = RelaxConv().export_tvm({\"forward\": {\"x\": relax.frontend.nn.spec.Tensor(input_shape, \"float32\")}})\n",
    "relax_mod = relax.transform.LegalizeOps()(relax_mod) # tvm ne veut pas compiler si on ne descend pas en tir manuellement\n",
    "# relax_mod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implémentation perso de la même convolution, en passant par des layouts différents\n",
    "\n",
    "\n",
    "@tvm.script.ir_module\n",
    "class MyConv:\n",
    "    @T.prim_func\n",
    "    def conv(X: T.Buffer((1, 1024, 15, 15), \"float32\"),\n",
    "             W: T.Buffer((1024, 3,3, 1024), \"float32\"),\n",
    "             Y: T.Buffer((1, 13,13, 1024), \"float32\")):\n",
    "        for di, dj, i, j, c_in, c_out in T.grid(3,3,13,13,1024,1024):\n",
    "            with T.block(\"out\"):\n",
    "                v_c_in, v_c_out, v_i, v_j, v_di, v_dj = T.axis.remap(\"RSSSRR\", (c_in, c_out, i, j, di, dj))\n",
    "                with T.init():\n",
    "                    Y[0, v_i, v_j, v_c_out] = 0\n",
    "                Y[0, v_i, v_j, v_c_out] = Y[0, v_i, v_j, v_c_out] + X[0, v_c_in, v_i+v_di, v_j+v_dj] * W[v_c_in, v_di, v_dj, v_c_out]\n",
    "\n",
    "    @R.function\n",
    "    def forward(X: R.Tensor((1, 1024, 15, 15), \"float32\"),\n",
    "                W: R.Tensor((1024, 1024, 3, 3), \"float32\")):\n",
    "        cls = MyConv\n",
    "        with R.dataflow():\n",
    "            # transposed_x = relax.op.permute_dims(X, [0,2,3,1])\n",
    "            transposed_w = relax.op.permute_dims(W, [0,2,3,1])\n",
    "            lv0 = R.call_tir(cls.conv, (X,transposed_w), out_sinfo=R.Tensor((1, 13, 13, 1024), dtype=\"float32\"))\n",
    "            lv1 = relax.op.permute_dims(lv0, [0,3,1,2])\n",
    "            R.output(lv1)\n",
    "        return lv1\n",
    "\n",
    "#my_conv = relax.transform.LegalizeOps()(MyConv)\n",
    "#my_conv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1 = relax.build(relax_mod, target=\"llvm -mcpu=core-avx2\")\n",
    "vm1 = relax.VirtualMachine(ex1, tvm.cpu(), profile=True)\n",
    "ex2 = relax.build(MyConv, target=\"llvm -mcpu=core-avx2\")\n",
    "vm2 = relax.VirtualMachine(ex2, tvm.cpu(), profile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on vérifie que la version custom fonctionne\n",
    "\n",
    "x = np.random.rand(1,1024,15,15).astype(\"float32\")\n",
    "tvm_x = tvm.nd.array(x)\n",
    "w = np.random.rand(1024,1024,3,3).astype(\"float32\")\n",
    "tvm_w = tvm.nd.array(w)\n",
    "\n",
    "res1 = vm1[\"forward\"](tvm_x, tvm_w).numpy()\n",
    "res2 = vm2[\"forward\"](tvm_x, tvm_w).numpy()\n",
    "np.testing.assert_allclose(res1, res2, atol=1e-4, rtol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name                          Duration (us)  Percent  Device  Count                                                                Argument Shapes  \n",
       "conv2d                         2 966 032,76   100.00    cpu0      1  float32[1, 1024, 15, 15], float32[1024, 1024, 3, 3], float32[1, 1024, 13, 13]  \n",
       "vm.builtin.check_tensor_info           6,77     0.00    cpu0      1                                                       float32[1, 1024, 15, 15]  \n",
       "vm.builtin.match_shape                 2,97     0.00    cpu0      1                                                       float32[1, 1024, 15, 15]  \n",
       "----------                                                                                                                                          \n",
       "Sum                            2 966 042,49   100.00              3                                                                                 \n",
       "Total                          2 966 121,69             cpu0      1                                                                                 \n",
       "\n",
       "Configuration\n",
       "-------------\n",
       "Number of threads: 2\n",
       "Executor: VM"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = vm1.profile(\"forward\",\n",
    "    tvm_x, tvm_w\n",
    ")\n",
    "evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name                          Duration (us)  Percent  Device  Count                                                                Argument Shapes  \n",
       "conv                             560 013,21    97.60    cpu0      1  float32[1, 1024, 15, 15], float32[1024, 3, 3, 1024], float32[1, 13, 13, 1024]  \n",
       "transpose                         13 102,19     2.28    cpu0      1                           float32[1024, 1024, 3, 3], float32[1024, 3, 3, 1024]  \n",
       "transpose1                           474,37     0.08    cpu0      1                             float32[1, 13, 13, 1024], float32[1, 1024, 13, 13]  \n",
       "vm.builtin.check_tensor_info           4,24     0.00    cpu0      1                                                       float32[1, 1024, 15, 15]  \n",
       "vm.builtin.match_shape                 2,42     0.00    cpu0      1                                                       float32[1, 1024, 15, 15]  \n",
       "vm.builtin.check_tensor_info           2,34     0.00    cpu0      1                                                      float32[1024, 1024, 3, 3]  \n",
       "vm.builtin.match_shape                 1,23     0.00    cpu0      1                                                      float32[1024, 1024, 3, 3]  \n",
       "----------                                                                                                                                          \n",
       "Sum                              573 600,00    99.97              7                                                                                 \n",
       "Total                            573 780,04             cpu0      1                                                                                 \n",
       "\n",
       "Configuration\n",
       "-------------\n",
       "Number of threads: 2\n",
       "Executor: VM"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on obtient un gain de vitesse entre x3 et x4 si on ignore la transposition du kernel\n",
    "\n",
    "evaluator = vm2.profile(\"forward\",\n",
    "    tvm_x, tvm_w\n",
    ")\n",
    "evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #A2F\">@I</span><span style=\"color: #A2F; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #00F; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">conv</span>(X: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">15</span>, <span style=\"color: #008000\">15</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), W: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">1024</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Y: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">13</span>, <span style=\"color: #008000\">13</span>, <span style=\"color: #008000\">1024</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>parallel(<span style=\"color: #008000\">13</span>):\n",
       "            <span style=\"color: #008000; font-weight: bold\">for</span> c_in, j, di, dj, c_out <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">13</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">1024</span>):\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;out&quot;</span>):\n",
       "                    v_c_in, v_c_out, v_i, v_j, v_di, v_dj <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;RSSSRR&quot;</span>, [c_in, c_out, i, j, di, dj])\n",
       "                    T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(X[<span style=\"color: #008000\">0</span>, v_c_in, v_i <span style=\"color: #A2F; font-weight: bold\">+</span> v_di, v_j <span style=\"color: #A2F; font-weight: bold\">+</span> v_dj], W[v_c_in, v_di, v_dj, v_c_out])\n",
       "                    T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(Y[<span style=\"color: #008000\">0</span>, v_i, v_j, v_c_out])\n",
       "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>init():\n",
       "                        Y[<span style=\"color: #008000\">0</span>, v_i, v_j, v_c_out] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
       "                    Y[<span style=\"color: #008000\">0</span>, v_i, v_j, v_c_out] <span style=\"color: #A2F; font-weight: bold\">=</span> Y[<span style=\"color: #008000\">0</span>, v_i, v_j, v_c_out] <span style=\"color: #A2F; font-weight: bold\">+</span> X[<span style=\"color: #008000\">0</span>, v_c_in, v_i <span style=\"color: #A2F; font-weight: bold\">+</span> v_di, v_j <span style=\"color: #A2F; font-weight: bold\">+</span> v_dj] <span style=\"color: #A2F; font-weight: bold\">*</span> W[v_c_in, v_di, v_dj, v_c_out]\n",
       "\n",
       "    <span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">forward</span>(X: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">15</span>, <span style=\"color: #008000\">15</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), W: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">13</span>, <span style=\"color: #008000\">13</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        cls <span style=\"color: #A2F; font-weight: bold\">=</span> Module\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
       "            transposed_w: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">1024</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>permute_dims(W, axes<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">1</span>])\n",
       "            lv0 <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>conv, (X, transposed_w), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">13</span>, <span style=\"color: #008000\">13</span>, <span style=\"color: #008000\">1024</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv1: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">13</span>, <span style=\"color: #008000\">13</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>permute_dims(lv0, axes<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2</span>])\n",
       "            R<span style=\"color: #A2F; font-weight: bold\">.</span>output(lv1)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> lv1\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name                          Duration (us)  Percent  Device  Count                                                                Argument Shapes  \n",
       "conv                             117 834,46    89.75    cpu0      1  float32[1, 1024, 15, 15], float32[1024, 3, 3, 1024], float32[1, 13, 13, 1024]  \n",
       "transpose                         12 756,29     9.72    cpu0      1                           float32[1024, 1024, 3, 3], float32[1024, 3, 3, 1024]  \n",
       "transpose1                           480,60     0.37    cpu0      1                             float32[1, 13, 13, 1024], float32[1, 1024, 13, 13]  \n",
       "vm.builtin.check_tensor_info           5,90     0.00    cpu0      1                                                       float32[1, 1024, 15, 15]  \n",
       "vm.builtin.match_shape                 4,27     0.00    cpu0      1                                                       float32[1, 1024, 15, 15]  \n",
       "vm.builtin.match_shape                 4,04     0.00    cpu0      1                                                      float32[1024, 1024, 3, 3]  \n",
       "vm.builtin.check_tensor_info           1,08     0.00    cpu0      1                                                      float32[1024, 1024, 3, 3]  \n",
       "----------                                                                                                                                          \n",
       "Sum                              131 086,65    99.85              7                                                                                 \n",
       "Total                            131 289,78             cpu0      1                                                                                 \n",
       "\n",
       "Configuration\n",
       "-------------\n",
       "Number of threads: 2\n",
       "Executor: VM"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "sch=tvm.tir.Schedule(MyConv)\n",
    "out = sch.get_block(\"out\", func_name=\"conv\")\n",
    "di, dj, i, j, c_in, c_out = sch.get_loops(out)\n",
    "\n",
    "sch.reorder(i, c_in, j, di, dj, c_out)\n",
    "\n",
    "sch.parallel(i)\n",
    "\n",
    "# on obtient des performances moins bonnes en unrollant/vectorisant\n",
    "\n",
    "#sch.unroll(c_out)\n",
    "#sch.vectorize(c_out)\n",
    "\n",
    "sch.mod.show()\n",
    "\n",
    "ex_ = relax.build(sch.mod, target=\"llvm -mcpu=core-avx2\")\n",
    "vm_ = relax.VirtualMachine(ex_, tvm.cpu(), profile=True)\n",
    "evaluator = vm_.profile(\"forward\",\n",
    "    tvm_x, tvm_w\n",
    ")\n",
    "\n",
    "evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 0.10340332984924316\n",
      "Mon schedule: 0.13658976554870605\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timeit\n",
    "import time\n",
    "\n",
    "\n",
    "x_ = torch.Tensor(x)\n",
    "w_ = torch.Tensor(w)\n",
    "\n",
    "#timeit.timeit(lambda: torch.nn.functional.conv2d(x_, w_), number=10)/10\n",
    "\n",
    "start = time.time()\n",
    "torch.nn.functional.conv2d(x_, w_)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Torch:\", end - start)\n",
    "\n",
    "start = time.time()\n",
    "vm_[\"forward\"](tvm_x, tvm_w)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Mon schedule:\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #A2F\">@main</span>(<span style=\"color: #A2F; font-weight: bold\">%</span>X: Tensor[(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">15</span>, <span style=\"color: #008000\">15</span>), float32], <span style=\"color: #A2F; font-weight: bold\">%</span>W: Tensor[(<span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), float32]) {\n",
       "  nn<span style=\"color: #A2F; font-weight: bold\">.</span>conv2d(<span style=\"color: #A2F; font-weight: bold\">%</span>X, <span style=\"color: #A2F; font-weight: bold\">%</span>W, padding<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>], channels<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000\">1024</span>, kernel_size<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>], out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "}\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Modèle relay pour la même convolution \n",
    "\n",
    "\n",
    "# Define the input shape (batch size, channels, height, width)\n",
    "input_shape = (1, 1024, 15, 15)\n",
    "# Define the kernel shape (output channels, input channels, kernel height, kernel width)\n",
    "kernel_shape = (1024, 1024, 3,3)\n",
    "\n",
    "# Create a random input tensor\n",
    "x_var = relay.var(\"X\", shape = input_shape)\n",
    "# Create a random weight tensor for the convolution\n",
    "w_var = relay.var(\"W\", shape = kernel_shape)\n",
    "\n",
    "# Define the convolution operation\n",
    "conv = relay.nn.conv2d(\n",
    "    x_var,\n",
    "    w_var,\n",
    "    strides=(1, 1),\n",
    "    padding=(0,0),\n",
    "    kernel_size=(3, 3),\n",
    "    channels=1024,\n",
    "    out_dtype=\"float32\"\n",
    ")\n",
    "\n",
    "# Create a Relay function\n",
    "func = relay.Function([x_var, w_var], conv)\n",
    "\n",
    "# Create a Relay module\n",
    "relay_mod = tvm.IRModule.from_expr(func)\n",
    "relay_mode = relay.transform.InferType()(relay_mod)\n",
    "\n",
    "# Print the Relay module\n",
    "relay_mod.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time summary:\n",
      " mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  \n",
      "  55.2157      55.0933      56.0975      54.6721       0.5027                  \n"
     ]
    }
   ],
   "source": [
    "# en spécifiant -avx2 à Relay on obtient mieux que torch\n",
    "\n",
    "from tvm.contrib import graph_executor\n",
    "\n",
    "dev = tvm.cpu()  # or tvm.cuda() for GPU\n",
    "lib = relay.build(relay_mod, target=\"llvm -mcpu=core-avx2\", params=None)\n",
    "graph_mod = graph_executor.GraphModule(lib[\"default\"](dev))\n",
    "\n",
    "print(graph_mod.benchmark(tvm.cpu(), number=5, repeat=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.autotvm.tuner import XGBTuner, GATuner, RandomTuner, GridSearchTuner\n",
    "from tvm import autotvm\n",
    "\n",
    "target = tvm.target.Target(\"llvm -mcpu=core-avx2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = autotvm.task.extract_from_program(func, target=target, params={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = autotvm.tuner.GATuner(tasks[0])\n",
    "\n",
    "measure_option = autotvm.measure_option(\n",
    "    builder=autotvm.LocalBuilder(),\n",
    "    runner=autotvm.LocalRunner(number=5, repeat=1, min_repeat_ms=100)\n",
    ")\n",
    "\n",
    "tuner.tune(\n",
    "        n_trial=10,\n",
    "        measure_option=measure_option,\n",
    "        callbacks=[autotvm.callback.log_to_file(f\"conv2d.log\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with autotvm.apply_history_best(f\"conv2d.log\"):\n",
    "    with tvm.transform.PassContext(opt_level=3):\n",
    "        lib = relay.build(func, target=target, params={})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time summary:\n",
      " mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  \n",
      "  56.4165      55.9922      59.5721      54.0611       1.7836                  \n"
     ]
    }
   ],
   "source": [
    "graph_mod = graph_executor.GraphModule(lib[\"default\"](dev))\n",
    "\n",
    "print(graph_mod.benchmark(tvm.cpu(), number=5, repeat=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvm-build-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
